<div>
  <p style="font-size:35px; color: #2dc937" align="justify"><strong>Free entry + 100,000 jpy championship prize !!</strong></p>
  <h2><strong>Overview</strong></h2>
  <p style="font-size:20px" align="justify">
    The <strong>6th ABC Challenge</strong> is the <strong>Activity Recognition of Nurse Training Activity</strong> using Skeleton and Video Dataset with <strong>Generative AI</strong>. Activity types are each task of Endotracheal suctioning.
  </p>
  <ul>
    <li style="font-size:20px" align="justify">The dataset will provide the <strong>skeleton data for training / testing</strong> and the <strong>video data only during the training</strong>.</li>
    <li style="font-size:20px" align="justify">The skeleton data include many samples which recognized <strong>only partial parts of the body due to camera location limitations</strong>.</li>
    <li style="font-size:20px" align="justify">Participants are required to use a <strong>Generative AI</strong> or a <strong>Large Language Model (LLMs)</strong> (hereafter, Generative AI) in a creative way.</li>
  </ul>
  <p style="font-size:20px" align="justify">
        Endotracheal suctioning (ES) is a necessary practice carried out in intensive care units. It involves the removal of pulmonary secretions from a patient with an artificial airway in place. The procedure is associated with complications and risks including bleeding, and infection. Therefore, there is a need to develop an activity recognition system that can ensure the safety of patients as well as reflect to improve their skills while they conduct this complicated procedure. Activity recognition can be used to aid nurses in better managing and increasing the quality of their work, as well as evaluate their performance when they conduct ES. The activity recognition is the initial stage to determine the order of actions and assess the nurse’s skills.
  </p>
  <p style="font-size:20px" align="justify">
    Participants are required to recognize activities based on skeletal data. Since the data collection was a practical experiment, camera locations were limited by not showing the face, the size of the room, etc. Therefore, it is not possible to recognize all of the body parts, and many skeleton data had missing body parts. Additionally, <strong>Generative AI</strong> has been a hot topic in recent years and its momentum will continue to increase. In order to explore the potential for use in the field of activity recognition, participants are required to utilize the <strong>Generative AIs</strong> in a creative way.
  </p>

  <h2><strong>Challenge Goal and Task</strong></h2>
  <p style="font-size:20px" align="justify">
    The goal of this challenge is to recognize <strong>9 activities</strong> in Endotracheal suctioning (ES) by using <strong>skeleton data for training / testing</strong> and <strong>video only for training</strong>. In this challenge, participants are required to use the <strong>Generative AIs</strong> in a creative way. For evaluation, we will consider the <strong>F1 score</strong> and the <strong>paper contents</strong>. We will take an average F1 score for all the subjects.
  </p>
  <p style="font-size:20px" align="justify">
    The data we are providing is a part of the dataset used in our previous work, entitled <strong>“Toward Recognizing Nursing Activity in Endotracheal Suctioning Using Video-based Pose Estimation”</strong> <strong>[1]</strong>. The authors of this work proposed an algorithm to define and track the main subject. Also, missing keypoints problems due to the performance of the pose estimation algorithm are improved by smoothing keypoints.
  </p>
  <p style="font-size:16px" align="justify">
    <strong>[1]</strong>
      Hoang Anh Vy Ngo, Quynh N Phuong Vu, Noriyo Colley, Shinji Ninomiya, Satoshi Kanai, Shunsuke Komizunai, Atsushi Konno, Misuzu Nakamura, Sozo Inoue: “Toward Recognizing Nursing Activity in Endotracheal Suctioning Using Video-based Pose Estimation”, The 5th International Conference on Activity and Behavior Computing, 2023, (Germany).
  </p>
  <h2><strong>Tutorial Webinar [CONCLUDED]</strong></h2>
    <p style="font-size:20px" align="justify">
        As part of this year's challenge, we organized tutorial webinars on January 17th, 2024 both in Japanese and English. Tutorial resources can be found in the following links:
    </p>
    <ul>
        <li style="font-size:20px" align="justify">Tutorial codes: Click <a href="https://abc-research.github.io/challenge2024/tutorial/#tcodes"><strong></strong>here</strong></a>.</li>
        <li style="font-size:20px" align="justify">Tutorial slide: Click <a href="https://abc-research.github.io/challenge2024/tutorial/#tslide"><strong></strong>here</strong></a>.</li>
        <li style="font-size:20px" align="justify">Tutorial video: Click <a href="https://abc-research.github.io/challenge2024/tutorial/#tvideo"><strong></strong>here</strong></a>.</li>
    </ul>

  <h2><strong>Challenge Registration and Dataset Distribution</strong></h2>
    <p style="font-size:20px" align="justify">
      You can register for the challenge from the website before the deadline. <span style="color: #f69801"><strong>Please note that the dataset will be published only to those who have registered for the Challenge at this time.</strong></span>
    </p>
    <br>
    <iframe style="pointer-events: none" align="justify" src="https://free.timeanddate.com/countdown/embed/i97ty7z7/n3399/cf12/cm0/cu4/ct0/cs1/ca0/cr0/ss0/cacf69801/cpcf69801/pct/tc66c/fs150/szw448/szh189/tatRegistration%20is%20now%20open%20!!/tac2dc937/tptRegistration%20is%20now%20closed%20!!/tpccc3232/matThe%20deadline%20is%20March%206%2C%202024%20(AoE)/maccc3232/mptThank%20you%20for%20your%20participation./mpc2dc937/iso2024-03-06T23:59:59/pa5" allowtransparency="true" frameborder="0" width="687" height="152"></iframe>
    <br>
    <br>
    <p><strong style="font-size:30px" align="justify">Click <a style="text-decoration: none; border: 0.2px solid #f69801; padding: 5px 10px; background: #f69801; border-radius: 15px;" href="https://docs.google.com/forms/d/e/1FAIpQLSctXaZJvL0oEN6BDLrbvu6IZ1tdPtU9ZWrtnC5w8i0tdZL8Mg/viewform?usp=sf_link"><span style="color: #1a1e23; filter: drop-shadow(.03em .03em #1a1e23);">HERE</span></a> to register for the challenge !!</strong></p>

    <h2><strong>Prizes</strong></h2>
    <ul>
      <li style="font-size:20px" align="justify">The winning team will be awarded <strong>100,000 jpy</strong>.</li>
      <li style="font-size:20px" align="justify">The registration fee for the 1st and 2nd runner-up teams will be waived.</li>
      <li style="font-size:20px" align="justify">Each of the participating teams will be awarded with participation certificate.</li>
    </ul>

  <h2><strong>Important dates</strong></h2>
  <p style="font-size:20px" align="justify">
  This Challenge will be held as part of the <strong><a href="https://abc-research.github.io">ABC Conference 2024</a></strong>
  </p>
  <ul>
    <li style="font-size:20px" align="justify"><del><strong>Tutorial:</strong> Jan 17, 2024 [CONCLUDED]</del></li>
    <li style="font-size:20px" align="justify"><strong>Challenge opens:</strong> Jan 17, 2024</li>
    <!-- <li style="font-size:20px" align="justify"><strong>Test data sent to participants:</strong> Feb 21, 2024</li> -->
    <li style="font-size:20px" align="justify"><strong>Registration closes:</strong> Mar 6, 2024</li>
    <li style="font-size:20px" align="justify"><strong>Submission of results:</strong> Mar 20, 2024</li>
    <li style="font-size:20px" align="justify"><strong>Submission of paper:</strong> Mar 27, 2024</li>
    <li style="font-size:20px" align="justify"><strong>Review sent to participants:</strong> Apr 10, 2024</li>
    <li style="font-size:20px" align="justify"><strong>Camera-ready papers:</strong> Apr 17, 2024</li>
    <li style="font-size:20px" align="justify"><strong>Conference:</strong> May 29 - 31, 2024</li>
  </ul>
</div>
