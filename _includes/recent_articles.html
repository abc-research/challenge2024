<div>
  <h1><strong>Overview</strong></h1>
  <hr style="border-color: #007bff"/>
  <p style="font-size:20px" align="justify">The <em><strong>6<sup>th</sup> ABC Challenge</strong></em> is the <em><strong>Activity Recognition of Nurse Training Activity</strong></em> using skelton and video dataset with <em><strong>Generative AI</strong></em>. Activity types are each task of Endotracheal suctioning.</p>
  <ul>
    <li style="font-size:20px" align="justify">The dataset will provide the <strong>skeleton data for training / testing</strong> and the <strong>video data only during the training</strong>.</li>
    <li style="font-size:20px" align="justify">The skeleton data include many samples which recognized <strong>only partial parts of the body due to camera location limitations</strong>.</li>
    <li style="font-size:20px" align="justify">Participants are required to use a <em><strong>Generative AI</strong></em> model or a <em><strong>Large Language Model (LLM)</strong></em> in a creative way.</li>
  </ul>
  <p style="font-size:20px" align="justify">Endotracheal suctioning (ES) is a necessary practice carried out in intensive care units. It involves the removal of pulmonary secretions from a patient with an artificial airway in place. The procedure is associated with complications and risks including bleeding, and infection. Therefore, there is a need to develop an activity recognition system that can ensure the safety of patients as well as reflect to improve their skills while they conduct this complicated procedure. Activity recognition can be used to aid nurses in better managing and increasing the quality of their work, as well as evaluate their performance when they conduct ES. The activity recognition is the initial stage to determine the order of actions and assess the nurse’s skills.</p>
  <p style="font-size:20px" align="justify">Participants are required to recognize activities based on skeletal data. Since the data collection was a practical experiment, camera locations were limited by not showing the face, the size of the room, etc. Therefore, it is not possible to recognize all of the body parts, and many skeleton data had missing body parts. Additionally, <em><strong>Generative AI</strong></em> has been a hot topic in recent years and its momentum will continue to increase. In order to explore the potential for use in the field of activity recognition, participants are required to utilize the <em><strong>Generative AI</strong></em> model or <em><strong>LLM</strong></em> in a creative way.</p>
  
  <h3><strong>Challenge Goal and Task</strong></h3>
  <hr style="border-color: #f69801"/>
  <p style="font-size:20px" align="justify">The goal of this challenge is to recognize <em><strong>9 activities in Endotracheal suctioning (ES)</strong></em> by using <strong>skeleton data for training / testing</strong> and <strong>video only for training</strong>. In this challenge, participants are required to use a <em><strong>Generative AI</strong></em> model or a <em><strong>LLM</strong></em> in a creative way. For evaluation, we will consider the <strong>F1</strong> score and the <strong>paper contents</strong>. We will take an <strong>average F1 score</strong> for all the subjects.</p>
  <p style="font-size:20px" align="justify">The data we are providing is a part of the dataset used in our previous work, entitled <em><strong>“Toward Recognizing Nursing Activity in Endotracheal Suctioning Using Video-based Pose Estimation”</strong></em> <strong>[1]</strong>. The authors of this work proposed an algorithm to define and track the main subject. Also, missing keypoints problems due to the performance of the pose estimation algorithm are improved by smoothing keypoints.</p>
  <p style="font-size:18px" align="justify"><strong>[1]</strong> <em>Hoang Anh Vy Ngo, Quynh N Phuong Vu, Noriyo Colley, Shinji Ninomiya, Satoshi Kanai, Shunsuke Komizunai, Atsushi Konno, Misuzu Nakamura, Sozo Inoue: “Toward Recognizing Nursing Activity in Endotracheal Suctioning Using Video-based Pose Estimation”, The 5th International Conference on Activity and Behavior Computing, 2023, (Germany).</em></p>
  
  <h3><strong>Important dates</strong></h3>
  <hr style="border-color: #f69801"/>
  <ul>
    <li style="font-size:20px" align="justify"><strong>Tutorials:</strong> <em>Jan 17, 2024</em></li>
    <li style="font-size:20px" align="justify"><strong>Challenge opens:</strong> <em>Jan 17, 2024</em></li>
    <li style="font-size:20px" align="justify"><strong>Test data sent to participants:</strong> <em>Feb 21, 2024</em></li>
    <li style="font-size:20px" align="justify"><strong>Registration closes:</strong> <em>Mar 6, 2024</em></li>
    <li style="font-size:20px" align="justify"><strong>Submission of results:</strong> <em>Mar 20, 2024</em></li>
    <li style="font-size:20px" align="justify"><strong>Submission of paper:</strong> <em>Mar 27, 2024</em></li>
    <li style="font-size:20px" align="justify"><strong>Review sent to participants:</strong> <em>Apr 10, 2024</em></li>
    <li style="font-size:20px" align="justify"><strong>Camera-ready papers:</strong> <em>Apr 17, 2024</em></li>
    <li style="font-size:20px" align="justify"><strong>Workshop:</strong> <em>May 29, 2024</em></li>
  </ul>
</div>